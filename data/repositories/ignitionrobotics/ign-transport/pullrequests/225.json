{"rendered": {"description": {"raw": "This PR adds a `bench` example program that can be used to evaluate the latency and throughput of `ign-transport`.\r\n\r\nA deadlock was found in `Node.cc` when using intraprocess communication with a mutex. The modification to `Node.cc` changes intraprocess message delivery from blocking to non-blocking. The behavior now matches interprocess communication. A few tests relied on blocking intraprocess message delivery. Those tests have been modified to use condition variables.\r\n\r\nA slight change was made to `NodeShared` to reduce the length of time the mutex is held during message publication. I didn't notice any performance impact, but there might be change with large numbers of publishers and subscribers.\r\n\r\n# Dependencies\r\n\r\nThis PR relies on C++14, and will not be merged until pull request #128 is merged first.\r\n\r\n# Benchmarking \r\n\r\nThis process could use more automation, but the scope started to get out of hand. Here are the steps to follow:\r\n\r\n1. Compile and install as normal.\r\n2. Compile the `examples`\r\n3. Run the `bench` example. In one of the following configurations:\r\n\r\n    1. Example intraprocess latency:\r\n\r\n            ./bench -l\r\n\r\n    2. Example interprocess latency:\r\n\r\n            Terminal 1: ./bench -l -r\r\n\r\n            Terminal 2: ./bench -l -p\r\n\r\n    3. Example intraprocess throughput:\r\n\r\n            ./bench -t\r\n\r\n    4. Example interprocess throughput:\r\n\r\n            Terminal 1: ./bench -t -r\r\n\r\n            Terminal 2: ./bench -t -p\r\n\r\n4. The `bench` executable with the `-p` option will output data suitable for use with the `latency.gp` and `throughput.gp` gnuplot scripts. Note: you can use the `-o` option with `bench` to output to a file.\r\n\r\n5. Produce plots using\r\n\r\n    1. Latency\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" latency.gp\r\n\r\n    2. Throughput\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" throughput.gp\r\n\r\n## Results\r\n\r\nRaw data is stored in `examples/data`. We can use this (or some other location) to hold a history of benchmark data. All results are the average of 100 iterations. These tests include time required for message serialization and deserialization.\r\n\r\nI did not make a test out `bench` because results are highly dependent on hardware, background processes, and network activity.\r\n\r\nA few definitions follow.\r\n\r\n** Intraprocess**:  Publisher and subscriber in same process.\r\n\r\n** Interprocess**: Publisher and subscriber in separate processes on the same PC.\r\n\r\n** Wired**: Publisher and subscriber on separate PCs connected via a wired LAN.\r\n\r\n**Wireless**: Publisher and subscriber on separate Pcs where one computer was connected to wireless access point and the other hardwired to the LAN.\r\n\r\n** Latency**: Latency was measure by publishing a message on `topicA`, and then waiting for a response on `topicB`. The time between publication and reception of the response was divided in half to compute the one-way latency time.\r\n\r\n**Throughput**: Throughput was measure by rapidly publishing N messages. Once the N messages have been published, the publisher waits for N responses. The time from start of publication to reception of all N messages is used to compute throughput. \r\n\r\n### Latency\r\n\r\n** Intraprocess latency**\r\n\r\nI was not expecting this result. My theory as to why the plot is not more constant is a combination of context switches, other processes, etc.\r\n\r\n![latency-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/1473127294-latency-Intraprocess-all.png)\r\n\r\n![latency-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3217563991-latency-Intraprocess-small.png)\r\n\r\n![latency-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/83318001-latency-Intraprocess-large.png)\r\n\r\n**Interprocess latency**\r\n\r\n![latency-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2668403778-latency-Interprocess-all.png)\r\n\r\n![latency-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3207293959-latency-Interprocess-small.png)\r\n\r\n![latency-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/556609639-latency-Interprocess-large.png)\r\n**Wired latency**\r\n\r\n![latency-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1861775847-latency-Wired-all.png)\r\n\r\n![latency-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3238222459-latency-Wired-small.png)\r\n\r\n![latency-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/2639846742-latency-Wired-large.png)\r\n\r\n** Wireless latency**\r\n\r\nYou may notice that the latency appears better than over the wired network. I believe this is because I ran the wireless tests when no one else was using the network, and the wired test during an active portion of the day.\r\n\r\n![latency-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/177746882-latency-Wireless-all.png)\r\n\r\n![latency-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/4212627213-latency-Wireless-small.png)\r\n\r\n![latency-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/3866951968-latency-Wireless-large.png)\r\n\r\n### Throughput\r\n\r\n** Intraprocess throughput**\r\n\r\n![throughput-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2829676984-throughput-Intraprocess-all.png)\r\n\r\n![throughput-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3933927565-throughput-Intraprocess-small.png)\r\n\r\n![throughput-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/3718225819-throughput-Intraprocess-large.png)\r\n\r\n** Interprocess throughput**\r\n\r\n![throughput-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/3032255927-throughput-Interprocess-all.png)\r\n\r\n![throughput-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/1568831296-throughput-Interprocess-small.png)\r\n\r\n![throughput-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/4114343739-throughput-Interprocess-large.png)\r\n\r\n** Wired throughput**\r\n\r\n![throughput-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1229007404-throughput-Wired-all.png)\r\n\r\n![throughput-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3679643025-throughput-Wired-small.png)\r\n\r\n![throughput-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/3619655927-throughput-Wired-large.png)\r\n\r\n** Wireless throughput**\r\n\r\n![throughput-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/365561471-throughput-Wireless-all.png)\r\n\r\n![throughput-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/2412795863-throughput-Wireless-small.png)\r\n\r\n![throughput-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/1670547079-throughput-Wireless-large.png)\r\n\r\n## Comparison\r\n\r\nI had no idea whether these results were good or bad. [This paper](https://dl.acm.org/citation.cfm?id=2968502) on ROS1 and ROS2 performance can be used as a point of comparison.", "markup": "markdown", "html": "<p>This PR adds a <code>bench</code> example program that can be used to evaluate the latency and throughput of <code>ign-transport</code>.</p>\n<p>A deadlock was found in <code>Node.cc</code> when using intraprocess communication with a mutex. The modification to <code>Node.cc</code> changes intraprocess message delivery from blocking to non-blocking. The behavior now matches interprocess communication. A few tests relied on blocking intraprocess message delivery. Those tests have been modified to use condition variables.</p>\n<p>A slight change was made to <code>NodeShared</code> to reduce the length of time the mutex is held during message publication. I didn't notice any performance impact, but there might be change with large numbers of publishers and subscribers.</p>\n<h1 id=\"markdown-header-dependencies\">Dependencies</h1>\n<p>This PR relies on C++14, and will not be merged until <a href=\"#!/ignitionrobotics/ign-transport/pull-requests/128/remove-duplicated-code-from-netutils\" rel=\"nofollow\" class=\"ap-connect-link\">pull request #128</a> is merged first.</p>\n<h1 id=\"markdown-header-benchmarking\">Benchmarking</h1>\n<p>This process could use more automation, but the scope started to get out of hand. Here are the steps to follow:</p>\n<ol>\n<li>Compile and install as normal.</li>\n<li>Compile the <code>examples</code></li>\n<li>\n<p>Run the <code>bench</code> example. In one of the following configurations:</p>\n<ol>\n<li>\n<p>Example intraprocess latency:</p>\n<div class=\"codehilite\"><pre><span></span>./bench -l\n</pre></div>\n\n\n</li>\n<li>\n<p>Example interprocess latency:</p>\n<div class=\"codehilite\"><pre><span></span>Terminal 1: ./bench -l -r\n\nTerminal 2: ./bench -l -p\n</pre></div>\n\n\n</li>\n<li>\n<p>Example intraprocess throughput:</p>\n<div class=\"codehilite\"><pre><span></span>./bench -t\n</pre></div>\n\n\n</li>\n<li>\n<p>Example interprocess throughput:</p>\n<div class=\"codehilite\"><pre><span></span>Terminal 1: ./bench -t -r\n\nTerminal 2: ./bench -t -p\n</pre></div>\n\n\n</li>\n</ol>\n</li>\n<li>\n<p>The <code>bench</code> executable with the <code>-p</code> option will output data suitable for use with the <code>latency.gp</code> and <code>throughput.gp</code> gnuplot scripts. Note: you can use the <code>-o</code> option with <code>bench</code> to output to a file.</p>\n</li>\n<li>\n<p>Produce plots using</p>\n<ol>\n<li>\n<p>Latency</p>\n<div class=\"codehilite\"><pre><span></span>gnuplot -e &quot;filename=&#39;MY_FILENAME&#39;; prefix=&#39;MY_PREFIX&#39;&quot; latency.gp\n</pre></div>\n\n\n</li>\n<li>\n<p>Throughput</p>\n<div class=\"codehilite\"><pre><span></span>gnuplot -e &quot;filename=&#39;MY_FILENAME&#39;; prefix=&#39;MY_PREFIX&#39;&quot; throughput.gp\n</pre></div>\n\n\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"markdown-header-results\">Results</h2>\n<p>Raw data is stored in <code>examples/data</code>. We can use this (or some other location) to hold a history of benchmark data. All results are the average of 100 iterations. These tests include time required for message serialization and deserialization.</p>\n<p>I did not make a test out <code>bench</code> because results are highly dependent on hardware, background processes, and network activity.</p>\n<p>A few definitions follow.</p>\n<p><strong> Intraprocess</strong>:  Publisher and subscriber in same process.</p>\n<p><strong> Interprocess</strong>: Publisher and subscriber in separate processes on the same PC.</p>\n<p><strong> Wired</strong>: Publisher and subscriber on separate PCs connected via a wired LAN.</p>\n<p><strong>Wireless</strong>: Publisher and subscriber on separate Pcs where one computer was connected to wireless access point and the other hardwired to the LAN.</p>\n<p><strong> Latency</strong>: Latency was measure by publishing a message on <code>topicA</code>, and then waiting for a response on <code>topicB</code>. The time between publication and reception of the response was divided in half to compute the one-way latency time.</p>\n<p><strong>Throughput</strong>: Throughput was measure by rapidly publishing N messages. Once the N messages have been published, the publisher waits for N responses. The time from start of publication to reception of all N messages is used to compute throughput. </p>\n<h3 id=\"markdown-header-latency\">Latency</h3>\n<p><strong> Intraprocess latency</strong></p>\n<p>I was not expecting this result. My theory as to why the plot is not more constant is a combination of context switches, other processes, etc.</p>\n<p><img alt=\"latency-Intraprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1473127294-latency-Intraprocess-all.png\" /></p>\n<p><img alt=\"latency-Intraprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3217563991-latency-Intraprocess-small.png\" /></p>\n<p><img alt=\"latency-Intraprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/83318001-latency-Intraprocess-large.png\" /></p>\n<p><strong>Interprocess latency</strong></p>\n<p><img alt=\"latency-Interprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2668403778-latency-Interprocess-all.png\" /></p>\n<p><img alt=\"latency-Interprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3207293959-latency-Interprocess-small.png\" /></p>\n<p><img alt=\"latency-Interprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/556609639-latency-Interprocess-large.png\" />\n<strong>Wired latency</strong></p>\n<p><img alt=\"latency-Wired-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1861775847-latency-Wired-all.png\" /></p>\n<p><img alt=\"latency-Wired-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3238222459-latency-Wired-small.png\" /></p>\n<p><img alt=\"latency-Wired-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2639846742-latency-Wired-large.png\" /></p>\n<p><strong> Wireless latency</strong></p>\n<p>You may notice that the latency appears better than over the wired network. I believe this is because I ran the wireless tests when no one else was using the network, and the wired test during an active portion of the day.</p>\n<p><img alt=\"latency-Wireless-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/177746882-latency-Wireless-all.png\" /></p>\n<p><img alt=\"latency-Wireless-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/4212627213-latency-Wireless-small.png\" /></p>\n<p><img alt=\"latency-Wireless-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3866951968-latency-Wireless-large.png\" /></p>\n<h3 id=\"markdown-header-throughput\">Throughput</h3>\n<p><strong> Intraprocess throughput</strong></p>\n<p><img alt=\"throughput-Intraprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2829676984-throughput-Intraprocess-all.png\" /></p>\n<p><img alt=\"throughput-Intraprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3933927565-throughput-Intraprocess-small.png\" /></p>\n<p><img alt=\"throughput-Intraprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3718225819-throughput-Intraprocess-large.png\" /></p>\n<p><strong> Interprocess throughput</strong></p>\n<p><img alt=\"throughput-Interprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3032255927-throughput-Interprocess-all.png\" /></p>\n<p><img alt=\"throughput-Interprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1568831296-throughput-Interprocess-small.png\" /></p>\n<p><img alt=\"throughput-Interprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/4114343739-throughput-Interprocess-large.png\" /></p>\n<p><strong> Wired throughput</strong></p>\n<p><img alt=\"throughput-Wired-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1229007404-throughput-Wired-all.png\" /></p>\n<p><img alt=\"throughput-Wired-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3679643025-throughput-Wired-small.png\" /></p>\n<p><img alt=\"throughput-Wired-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3619655927-throughput-Wired-large.png\" /></p>\n<p><strong> Wireless throughput</strong></p>\n<p><img alt=\"throughput-Wireless-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/365561471-throughput-Wireless-all.png\" /></p>\n<p><img alt=\"throughput-Wireless-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2412795863-throughput-Wireless-small.png\" /></p>\n<p><img alt=\"throughput-Wireless-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1670547079-throughput-Wireless-large.png\" /></p>\n<h2 id=\"markdown-header-comparison\">Comparison</h2>\n<p>I had no idea whether these results were good or bad. <a data-is-external-link=\"true\" href=\"https://dl.acm.org/citation.cfm?id=2968502\" rel=\"nofollow\">This paper</a> on ROS1 and ROS2 performance can be used as a point of comparison.</p>", "type": "rendered"}, "title": {"raw": "Benchmark", "markup": "markdown", "html": "<p>Benchmark</p>", "type": "rendered"}}, "type": "pullrequest", "description": "This PR adds a `bench` example program that can be used to evaluate the latency and throughput of `ign-transport`.\r\n\r\nA deadlock was found in `Node.cc` when using intraprocess communication with a mutex. The modification to `Node.cc` changes intraprocess message delivery from blocking to non-blocking. The behavior now matches interprocess communication. A few tests relied on blocking intraprocess message delivery. Those tests have been modified to use condition variables.\r\n\r\nA slight change was made to `NodeShared` to reduce the length of time the mutex is held during message publication. I didn't notice any performance impact, but there might be change with large numbers of publishers and subscribers.\r\n\r\n# Dependencies\r\n\r\nThis PR relies on C++14, and will not be merged until pull request #128 is merged first.\r\n\r\n# Benchmarking \r\n\r\nThis process could use more automation, but the scope started to get out of hand. Here are the steps to follow:\r\n\r\n1. Compile and install as normal.\r\n2. Compile the `examples`\r\n3. Run the `bench` example. In one of the following configurations:\r\n\r\n    1. Example intraprocess latency:\r\n\r\n            ./bench -l\r\n\r\n    2. Example interprocess latency:\r\n\r\n            Terminal 1: ./bench -l -r\r\n\r\n            Terminal 2: ./bench -l -p\r\n\r\n    3. Example intraprocess throughput:\r\n\r\n            ./bench -t\r\n\r\n    4. Example interprocess throughput:\r\n\r\n            Terminal 1: ./bench -t -r\r\n\r\n            Terminal 2: ./bench -t -p\r\n\r\n4. The `bench` executable with the `-p` option will output data suitable for use with the `latency.gp` and `throughput.gp` gnuplot scripts. Note: you can use the `-o` option with `bench` to output to a file.\r\n\r\n5. Produce plots using\r\n\r\n    1. Latency\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" latency.gp\r\n\r\n    2. Throughput\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" throughput.gp\r\n\r\n## Results\r\n\r\nRaw data is stored in `examples/data`. We can use this (or some other location) to hold a history of benchmark data. All results are the average of 100 iterations. These tests include time required for message serialization and deserialization.\r\n\r\nI did not make a test out `bench` because results are highly dependent on hardware, background processes, and network activity.\r\n\r\nA few definitions follow.\r\n\r\n** Intraprocess**:  Publisher and subscriber in same process.\r\n\r\n** Interprocess**: Publisher and subscriber in separate processes on the same PC.\r\n\r\n** Wired**: Publisher and subscriber on separate PCs connected via a wired LAN.\r\n\r\n**Wireless**: Publisher and subscriber on separate Pcs where one computer was connected to wireless access point and the other hardwired to the LAN.\r\n\r\n** Latency**: Latency was measure by publishing a message on `topicA`, and then waiting for a response on `topicB`. The time between publication and reception of the response was divided in half to compute the one-way latency time.\r\n\r\n**Throughput**: Throughput was measure by rapidly publishing N messages. Once the N messages have been published, the publisher waits for N responses. The time from start of publication to reception of all N messages is used to compute throughput. \r\n\r\n### Latency\r\n\r\n** Intraprocess latency**\r\n\r\nI was not expecting this result. My theory as to why the plot is not more constant is a combination of context switches, other processes, etc.\r\n\r\n![latency-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/1473127294-latency-Intraprocess-all.png)\r\n\r\n![latency-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3217563991-latency-Intraprocess-small.png)\r\n\r\n![latency-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/83318001-latency-Intraprocess-large.png)\r\n\r\n**Interprocess latency**\r\n\r\n![latency-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2668403778-latency-Interprocess-all.png)\r\n\r\n![latency-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3207293959-latency-Interprocess-small.png)\r\n\r\n![latency-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/556609639-latency-Interprocess-large.png)\r\n**Wired latency**\r\n\r\n![latency-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1861775847-latency-Wired-all.png)\r\n\r\n![latency-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3238222459-latency-Wired-small.png)\r\n\r\n![latency-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/2639846742-latency-Wired-large.png)\r\n\r\n** Wireless latency**\r\n\r\nYou may notice that the latency appears better than over the wired network. I believe this is because I ran the wireless tests when no one else was using the network, and the wired test during an active portion of the day.\r\n\r\n![latency-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/177746882-latency-Wireless-all.png)\r\n\r\n![latency-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/4212627213-latency-Wireless-small.png)\r\n\r\n![latency-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/3866951968-latency-Wireless-large.png)\r\n\r\n### Throughput\r\n\r\n** Intraprocess throughput**\r\n\r\n![throughput-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2829676984-throughput-Intraprocess-all.png)\r\n\r\n![throughput-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3933927565-throughput-Intraprocess-small.png)\r\n\r\n![throughput-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/3718225819-throughput-Intraprocess-large.png)\r\n\r\n** Interprocess throughput**\r\n\r\n![throughput-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/3032255927-throughput-Interprocess-all.png)\r\n\r\n![throughput-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/1568831296-throughput-Interprocess-small.png)\r\n\r\n![throughput-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/4114343739-throughput-Interprocess-large.png)\r\n\r\n** Wired throughput**\r\n\r\n![throughput-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1229007404-throughput-Wired-all.png)\r\n\r\n![throughput-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3679643025-throughput-Wired-small.png)\r\n\r\n![throughput-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/3619655927-throughput-Wired-large.png)\r\n\r\n** Wireless throughput**\r\n\r\n![throughput-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/365561471-throughput-Wireless-all.png)\r\n\r\n![throughput-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/2412795863-throughput-Wireless-small.png)\r\n\r\n![throughput-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/1670547079-throughput-Wireless-large.png)\r\n\r\n## Comparison\r\n\r\nI had no idea whether these results were good or bad. [This paper](https://dl.acm.org/citation.cfm?id=2968502) on ROS1 and ROS2 performance can be used as a point of comparison.", "links": {"decline": {"href": "https://api.bitbucket.org/2.0/repositories/ignitionrobotics/ign-transport/pullrequests/225/decline"}, "diffstat": {"href": "https://api.bitbucket.org/2.0/repositories/ignitionrobotics/ign-transport/diffstat/ignitionrobotics/ign-transport:63a65837af55%0D817abf1ddb9c?from_pullrequest_id=225"}, "commits": {"href": "data/repositories/ignitionrobotics/ign-transport/pullrequests/225/commits.json"}, "self": {"href": "data/repositories/ignitionrobotics/ign-transport/pullrequests/225.json"}, "comments": {"href": "data/repositories/ignitionrobotics/ign-transport/pullrequests/225/comments_page=1.json"}, "merge": {"href": "https://api.bitbucket.org/2.0/repositories/ignitionrobotics/ign-transport/pullrequests/225/merge"}, "html": {"href": "#!/ignitionrobotics/ign-transport/pull-requests/225"}, "activity": {"href": "data/repositories/ignitionrobotics/ign-transport/pullrequests/225/activity.json"}, "diff": {"href": "https://api.bitbucket.org/2.0/repositories/ignitionrobotics/ign-transport/diff/ignitionrobotics/ign-transport:63a65837af55%0D817abf1ddb9c?from_pullrequest_id=225"}, "approve": {"href": "https://api.bitbucket.org/2.0/repositories/ignitionrobotics/ign-transport/pullrequests/225/approve"}, "statuses": {"href": "data/repositories/ignitionrobotics/ign-transport/pullrequests/225/statuses_page=1.json"}}, "title": "Benchmark", "close_source_branch": true, "reviewers": [], "id": 225, "destination": {"commit": {"hash": "817abf1ddb9c", "type": "commit", "links": {"self": {"href": "data/repositories/ignitionrobotics/ign-transport/commit/817abf1ddb9c.json"}, "html": {"href": "#!/ignitionrobotics/ign-transport/commits/817abf1ddb9c"}}}, "repository": {"links": {"self": {"href": "data/repositories/ignitionrobotics/ign-transport.json"}, "html": {"href": "#!/ignitionrobotics/ign-transport"}, "avatar": {"href": "data/bytebucket.org/ravatar/{4249390a-7f55-404b-990d-817ba94cc7ac}ts=1533306"}}, "type": "repository", "name": "ign-transport", "full_name": "ignitionrobotics/ign-transport", "uuid": "{4249390a-7f55-404b-990d-817ba94cc7ac}"}, "branch": {"name": "ign-transport4"}}, "created_on": "2017-09-26T23:08:05.419977+00:00", "summary": {"raw": "This PR adds a `bench` example program that can be used to evaluate the latency and throughput of `ign-transport`.\r\n\r\nA deadlock was found in `Node.cc` when using intraprocess communication with a mutex. The modification to `Node.cc` changes intraprocess message delivery from blocking to non-blocking. The behavior now matches interprocess communication. A few tests relied on blocking intraprocess message delivery. Those tests have been modified to use condition variables.\r\n\r\nA slight change was made to `NodeShared` to reduce the length of time the mutex is held during message publication. I didn't notice any performance impact, but there might be change with large numbers of publishers and subscribers.\r\n\r\n# Dependencies\r\n\r\nThis PR relies on C++14, and will not be merged until pull request #128 is merged first.\r\n\r\n# Benchmarking \r\n\r\nThis process could use more automation, but the scope started to get out of hand. Here are the steps to follow:\r\n\r\n1. Compile and install as normal.\r\n2. Compile the `examples`\r\n3. Run the `bench` example. In one of the following configurations:\r\n\r\n    1. Example intraprocess latency:\r\n\r\n            ./bench -l\r\n\r\n    2. Example interprocess latency:\r\n\r\n            Terminal 1: ./bench -l -r\r\n\r\n            Terminal 2: ./bench -l -p\r\n\r\n    3. Example intraprocess throughput:\r\n\r\n            ./bench -t\r\n\r\n    4. Example interprocess throughput:\r\n\r\n            Terminal 1: ./bench -t -r\r\n\r\n            Terminal 2: ./bench -t -p\r\n\r\n4. The `bench` executable with the `-p` option will output data suitable for use with the `latency.gp` and `throughput.gp` gnuplot scripts. Note: you can use the `-o` option with `bench` to output to a file.\r\n\r\n5. Produce plots using\r\n\r\n    1. Latency\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" latency.gp\r\n\r\n    2. Throughput\r\n\r\n            gnuplot -e \"filename='MY_FILENAME'; prefix='MY_PREFIX'\" throughput.gp\r\n\r\n## Results\r\n\r\nRaw data is stored in `examples/data`. We can use this (or some other location) to hold a history of benchmark data. All results are the average of 100 iterations. These tests include time required for message serialization and deserialization.\r\n\r\nI did not make a test out `bench` because results are highly dependent on hardware, background processes, and network activity.\r\n\r\nA few definitions follow.\r\n\r\n** Intraprocess**:  Publisher and subscriber in same process.\r\n\r\n** Interprocess**: Publisher and subscriber in separate processes on the same PC.\r\n\r\n** Wired**: Publisher and subscriber on separate PCs connected via a wired LAN.\r\n\r\n**Wireless**: Publisher and subscriber on separate Pcs where one computer was connected to wireless access point and the other hardwired to the LAN.\r\n\r\n** Latency**: Latency was measure by publishing a message on `topicA`, and then waiting for a response on `topicB`. The time between publication and reception of the response was divided in half to compute the one-way latency time.\r\n\r\n**Throughput**: Throughput was measure by rapidly publishing N messages. Once the N messages have been published, the publisher waits for N responses. The time from start of publication to reception of all N messages is used to compute throughput. \r\n\r\n### Latency\r\n\r\n** Intraprocess latency**\r\n\r\nI was not expecting this result. My theory as to why the plot is not more constant is a combination of context switches, other processes, etc.\r\n\r\n![latency-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/1473127294-latency-Intraprocess-all.png)\r\n\r\n![latency-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3217563991-latency-Intraprocess-small.png)\r\n\r\n![latency-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/83318001-latency-Intraprocess-large.png)\r\n\r\n**Interprocess latency**\r\n\r\n![latency-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2668403778-latency-Interprocess-all.png)\r\n\r\n![latency-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3207293959-latency-Interprocess-small.png)\r\n\r\n![latency-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/556609639-latency-Interprocess-large.png)\r\n**Wired latency**\r\n\r\n![latency-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1861775847-latency-Wired-all.png)\r\n\r\n![latency-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3238222459-latency-Wired-small.png)\r\n\r\n![latency-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/2639846742-latency-Wired-large.png)\r\n\r\n** Wireless latency**\r\n\r\nYou may notice that the latency appears better than over the wired network. I believe this is because I ran the wireless tests when no one else was using the network, and the wired test during an active portion of the day.\r\n\r\n![latency-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/177746882-latency-Wireless-all.png)\r\n\r\n![latency-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/4212627213-latency-Wireless-small.png)\r\n\r\n![latency-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/3866951968-latency-Wireless-large.png)\r\n\r\n### Throughput\r\n\r\n** Intraprocess throughput**\r\n\r\n![throughput-Intraprocess-all.png](data/bitbucket.org/repo/og7EBq/images/2829676984-throughput-Intraprocess-all.png)\r\n\r\n![throughput-Intraprocess-small.png](data/bitbucket.org/repo/og7EBq/images/3933927565-throughput-Intraprocess-small.png)\r\n\r\n![throughput-Intraprocess-large.png](data/bitbucket.org/repo/og7EBq/images/3718225819-throughput-Intraprocess-large.png)\r\n\r\n** Interprocess throughput**\r\n\r\n![throughput-Interprocess-all.png](data/bitbucket.org/repo/og7EBq/images/3032255927-throughput-Interprocess-all.png)\r\n\r\n![throughput-Interprocess-small.png](data/bitbucket.org/repo/og7EBq/images/1568831296-throughput-Interprocess-small.png)\r\n\r\n![throughput-Interprocess-large.png](data/bitbucket.org/repo/og7EBq/images/4114343739-throughput-Interprocess-large.png)\r\n\r\n** Wired throughput**\r\n\r\n![throughput-Wired-all.png](data/bitbucket.org/repo/og7EBq/images/1229007404-throughput-Wired-all.png)\r\n\r\n![throughput-Wired-small.png](data/bitbucket.org/repo/og7EBq/images/3679643025-throughput-Wired-small.png)\r\n\r\n![throughput-Wired-large.png](data/bitbucket.org/repo/og7EBq/images/3619655927-throughput-Wired-large.png)\r\n\r\n** Wireless throughput**\r\n\r\n![throughput-Wireless-all.png](data/bitbucket.org/repo/og7EBq/images/365561471-throughput-Wireless-all.png)\r\n\r\n![throughput-Wireless-small.png](data/bitbucket.org/repo/og7EBq/images/2412795863-throughput-Wireless-small.png)\r\n\r\n![throughput-Wireless-large.png](data/bitbucket.org/repo/og7EBq/images/1670547079-throughput-Wireless-large.png)\r\n\r\n## Comparison\r\n\r\nI had no idea whether these results were good or bad. [This paper](https://dl.acm.org/citation.cfm?id=2968502) on ROS1 and ROS2 performance can be used as a point of comparison.", "markup": "markdown", "html": "<p>This PR adds a <code>bench</code> example program that can be used to evaluate the latency and throughput of <code>ign-transport</code>.</p>\n<p>A deadlock was found in <code>Node.cc</code> when using intraprocess communication with a mutex. The modification to <code>Node.cc</code> changes intraprocess message delivery from blocking to non-blocking. The behavior now matches interprocess communication. A few tests relied on blocking intraprocess message delivery. Those tests have been modified to use condition variables.</p>\n<p>A slight change was made to <code>NodeShared</code> to reduce the length of time the mutex is held during message publication. I didn't notice any performance impact, but there might be change with large numbers of publishers and subscribers.</p>\n<h1 id=\"markdown-header-dependencies\">Dependencies</h1>\n<p>This PR relies on C++14, and will not be merged until <a href=\"#!/ignitionrobotics/ign-transport/pull-requests/128/remove-duplicated-code-from-netutils\" rel=\"nofollow\" class=\"ap-connect-link\">pull request #128</a> is merged first.</p>\n<h1 id=\"markdown-header-benchmarking\">Benchmarking</h1>\n<p>This process could use more automation, but the scope started to get out of hand. Here are the steps to follow:</p>\n<ol>\n<li>Compile and install as normal.</li>\n<li>Compile the <code>examples</code></li>\n<li>\n<p>Run the <code>bench</code> example. In one of the following configurations:</p>\n<ol>\n<li>\n<p>Example intraprocess latency:</p>\n<div class=\"codehilite\"><pre><span></span>./bench -l\n</pre></div>\n\n\n</li>\n<li>\n<p>Example interprocess latency:</p>\n<div class=\"codehilite\"><pre><span></span>Terminal 1: ./bench -l -r\n\nTerminal 2: ./bench -l -p\n</pre></div>\n\n\n</li>\n<li>\n<p>Example intraprocess throughput:</p>\n<div class=\"codehilite\"><pre><span></span>./bench -t\n</pre></div>\n\n\n</li>\n<li>\n<p>Example interprocess throughput:</p>\n<div class=\"codehilite\"><pre><span></span>Terminal 1: ./bench -t -r\n\nTerminal 2: ./bench -t -p\n</pre></div>\n\n\n</li>\n</ol>\n</li>\n<li>\n<p>The <code>bench</code> executable with the <code>-p</code> option will output data suitable for use with the <code>latency.gp</code> and <code>throughput.gp</code> gnuplot scripts. Note: you can use the <code>-o</code> option with <code>bench</code> to output to a file.</p>\n</li>\n<li>\n<p>Produce plots using</p>\n<ol>\n<li>\n<p>Latency</p>\n<div class=\"codehilite\"><pre><span></span>gnuplot -e &quot;filename=&#39;MY_FILENAME&#39;; prefix=&#39;MY_PREFIX&#39;&quot; latency.gp\n</pre></div>\n\n\n</li>\n<li>\n<p>Throughput</p>\n<div class=\"codehilite\"><pre><span></span>gnuplot -e &quot;filename=&#39;MY_FILENAME&#39;; prefix=&#39;MY_PREFIX&#39;&quot; throughput.gp\n</pre></div>\n\n\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"markdown-header-results\">Results</h2>\n<p>Raw data is stored in <code>examples/data</code>. We can use this (or some other location) to hold a history of benchmark data. All results are the average of 100 iterations. These tests include time required for message serialization and deserialization.</p>\n<p>I did not make a test out <code>bench</code> because results are highly dependent on hardware, background processes, and network activity.</p>\n<p>A few definitions follow.</p>\n<p><strong> Intraprocess</strong>:  Publisher and subscriber in same process.</p>\n<p><strong> Interprocess</strong>: Publisher and subscriber in separate processes on the same PC.</p>\n<p><strong> Wired</strong>: Publisher and subscriber on separate PCs connected via a wired LAN.</p>\n<p><strong>Wireless</strong>: Publisher and subscriber on separate Pcs where one computer was connected to wireless access point and the other hardwired to the LAN.</p>\n<p><strong> Latency</strong>: Latency was measure by publishing a message on <code>topicA</code>, and then waiting for a response on <code>topicB</code>. The time between publication and reception of the response was divided in half to compute the one-way latency time.</p>\n<p><strong>Throughput</strong>: Throughput was measure by rapidly publishing N messages. Once the N messages have been published, the publisher waits for N responses. The time from start of publication to reception of all N messages is used to compute throughput. </p>\n<h3 id=\"markdown-header-latency\">Latency</h3>\n<p><strong> Intraprocess latency</strong></p>\n<p>I was not expecting this result. My theory as to why the plot is not more constant is a combination of context switches, other processes, etc.</p>\n<p><img alt=\"latency-Intraprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1473127294-latency-Intraprocess-all.png\" /></p>\n<p><img alt=\"latency-Intraprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3217563991-latency-Intraprocess-small.png\" /></p>\n<p><img alt=\"latency-Intraprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/83318001-latency-Intraprocess-large.png\" /></p>\n<p><strong>Interprocess latency</strong></p>\n<p><img alt=\"latency-Interprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2668403778-latency-Interprocess-all.png\" /></p>\n<p><img alt=\"latency-Interprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3207293959-latency-Interprocess-small.png\" /></p>\n<p><img alt=\"latency-Interprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/556609639-latency-Interprocess-large.png\" />\n<strong>Wired latency</strong></p>\n<p><img alt=\"latency-Wired-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1861775847-latency-Wired-all.png\" /></p>\n<p><img alt=\"latency-Wired-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3238222459-latency-Wired-small.png\" /></p>\n<p><img alt=\"latency-Wired-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2639846742-latency-Wired-large.png\" /></p>\n<p><strong> Wireless latency</strong></p>\n<p>You may notice that the latency appears better than over the wired network. I believe this is because I ran the wireless tests when no one else was using the network, and the wired test during an active portion of the day.</p>\n<p><img alt=\"latency-Wireless-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/177746882-latency-Wireless-all.png\" /></p>\n<p><img alt=\"latency-Wireless-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/4212627213-latency-Wireless-small.png\" /></p>\n<p><img alt=\"latency-Wireless-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3866951968-latency-Wireless-large.png\" /></p>\n<h3 id=\"markdown-header-throughput\">Throughput</h3>\n<p><strong> Intraprocess throughput</strong></p>\n<p><img alt=\"throughput-Intraprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2829676984-throughput-Intraprocess-all.png\" /></p>\n<p><img alt=\"throughput-Intraprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3933927565-throughput-Intraprocess-small.png\" /></p>\n<p><img alt=\"throughput-Intraprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3718225819-throughput-Intraprocess-large.png\" /></p>\n<p><strong> Interprocess throughput</strong></p>\n<p><img alt=\"throughput-Interprocess-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3032255927-throughput-Interprocess-all.png\" /></p>\n<p><img alt=\"throughput-Interprocess-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1568831296-throughput-Interprocess-small.png\" /></p>\n<p><img alt=\"throughput-Interprocess-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/4114343739-throughput-Interprocess-large.png\" /></p>\n<p><strong> Wired throughput</strong></p>\n<p><img alt=\"throughput-Wired-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1229007404-throughput-Wired-all.png\" /></p>\n<p><img alt=\"throughput-Wired-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3679643025-throughput-Wired-small.png\" /></p>\n<p><img alt=\"throughput-Wired-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/3619655927-throughput-Wired-large.png\" /></p>\n<p><strong> Wireless throughput</strong></p>\n<p><img alt=\"throughput-Wireless-all.png\" src=\"data/bitbucket.org/repo/og7EBq/images/365561471-throughput-Wireless-all.png\" /></p>\n<p><img alt=\"throughput-Wireless-small.png\" src=\"data/bitbucket.org/repo/og7EBq/images/2412795863-throughput-Wireless-small.png\" /></p>\n<p><img alt=\"throughput-Wireless-large.png\" src=\"data/bitbucket.org/repo/og7EBq/images/1670547079-throughput-Wireless-large.png\" /></p>\n<h2 id=\"markdown-header-comparison\">Comparison</h2>\n<p>I had no idea whether these results were good or bad. <a data-is-external-link=\"true\" href=\"https://dl.acm.org/citation.cfm?id=2968502\" rel=\"nofollow\">This paper</a> on ROS1 and ROS2 performance can be used as a point of comparison.</p>", "type": "rendered"}, "source": {"commit": {"hash": "8de5cfec6607", "type": "commit", "links": {"self": {"href": "data/repositories/ignitionrobotics/ign-transport/commit/8de5cfec6607.json"}, "html": {"href": "#!/ignitionrobotics/ign-transport/commits/8de5cfec6607"}}}, "repository": {"links": {"self": {"href": "data/repositories/ignitionrobotics/ign-transport.json"}, "html": {"href": "#!/ignitionrobotics/ign-transport"}, "avatar": {"href": "data/bytebucket.org/ravatar/{4249390a-7f55-404b-990d-817ba94cc7ac}ts=1533306"}}, "type": "repository", "name": "ign-transport", "full_name": "ignitionrobotics/ign-transport", "uuid": "{4249390a-7f55-404b-990d-817ba94cc7ac}"}, "branch": {"name": "benchmark_nate"}}, "comment_count": 48, "state": "MERGED", "task_count": 0, "participants": [{"role": "PARTICIPANT", "participated_on": "2017-10-02T17:52:18.693926+00:00", "type": "participant", "approved": false, "user": {"display_name": "Shane Loretz", "uuid": "{656e3311-aad9-45a1-aaf7-b0ee0e84b287}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B656e3311-aad9-45a1-aaf7-b0ee0e84b287%7D"}, "html": {"href": "https://bitbucket.org/%7B656e3311-aad9-45a1-aaf7-b0ee0e84b287%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:684383ab-ac95-4859-a350-4a6f41a94a22/c7a1ebf5-cade-4115-9f26-9d3facb776db/128"}}, "nickname": "Shane Loretz", "type": "user", "account_id": "557058:684383ab-ac95-4859-a350-4a6f41a94a22"}}, {"role": "PARTICIPANT", "participated_on": "2018-01-11T00:21:34.152670+00:00", "type": "participant", "approved": false, "user": {"display_name": "Nate Koenig", "uuid": "{c862cdd9-fcc8-4419-9fc7-e20db14b8fcb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D"}, "html": {"href": "https://bitbucket.org/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:095b1e12-74ed-4e20-b44f-2f0745b616e0/ca24bb11-4787-4b14-a20d-d91835e9bde2/128"}}, "nickname": "Nathan Koenig", "type": "user", "account_id": "557058:095b1e12-74ed-4e20-b44f-2f0745b616e0"}}, {"role": "PARTICIPANT", "participated_on": "2018-01-04T21:46:33.941699+00:00", "type": "participant", "approved": true, "user": {"display_name": "Steve Peters", "uuid": "{2ccfed09-18b8-4921-8d58-15ef01092802}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D"}, "html": {"href": "https://bitbucket.org/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/1fb4816dad9e68337d3096f750951f6cd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsSP-1.png"}}, "nickname": "Steven Peters", "type": "user", "account_id": "557058:5de38267-b118-494c-aa76-4fab35448816"}}, {"role": "PARTICIPANT", "participated_on": "2017-10-09T21:05:13.916346+00:00", "type": "participant", "approved": false, "user": {"display_name": "Carlos Ag\u00fcero", "uuid": "{da8a8e89-4bb0-421b-bd0e-dbbed3d4ed6a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bda8a8e89-4bb0-421b-bd0e-dbbed3d4ed6a%7D"}, "html": {"href": "https://bitbucket.org/%7Bda8a8e89-4bb0-421b-bd0e-dbbed3d4ed6a%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/692bf15758111acaddae4da15a47f9e5d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsCA-0.png"}}, "nickname": "caguero", "type": "user", "account_id": "557058:4ded1ddf-947e-4154-bbd1-3dba24f1bdbd"}}, {"role": "PARTICIPANT", "participated_on": "2018-01-10T23:42:32.639900+00:00", "type": "participant", "approved": false, "user": {"display_name": "Louise Poubel", "uuid": "{5cfa2075-477b-4ded-bdb9-8d2479544ec4}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5cfa2075-477b-4ded-bdb9-8d2479544ec4%7D"}, "html": {"href": "https://bitbucket.org/%7B5cfa2075-477b-4ded-bdb9-8d2479544ec4%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:6ff86fcb-b7ab-44a5-b8a6-f6d9cae8b6e8/7d903d90-c5ea-4182-b7ef-0d467e9e1c74/128"}}, "nickname": "chapulina", "type": "user", "account_id": "557058:6ff86fcb-b7ab-44a5-b8a6-f6d9cae8b6e8"}}, {"role": "PARTICIPANT", "participated_on": "2018-01-11T01:14:42.184291+00:00", "type": "participant", "approved": true, "user": {"display_name": "Michael Grey", "uuid": "{c1cdfe52-2887-474c-ae99-72fdc53a59c9}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bc1cdfe52-2887-474c-ae99-72fdc53a59c9%7D"}, "html": {"href": "https://bitbucket.org/%7Bc1cdfe52-2887-474c-ae99-72fdc53a59c9%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:67759e29-d1df-465f-868d-047738e36fc9/d42d74cf-c1bd-4431-8288-07f543bbe325/128"}}, "nickname": "mxgrey", "type": "user", "account_id": "557058:67759e29-d1df-465f-868d-047738e36fc9"}}], "reason": "", "updated_on": "2018-01-11T18:21:15.015731+00:00", "author": {"display_name": "Nate Koenig", "uuid": "{c862cdd9-fcc8-4419-9fc7-e20db14b8fcb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D"}, "html": {"href": "https://bitbucket.org/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:095b1e12-74ed-4e20-b44f-2f0745b616e0/ca24bb11-4787-4b14-a20d-d91835e9bde2/128"}}, "nickname": "Nathan Koenig", "type": "user", "account_id": "557058:095b1e12-74ed-4e20-b44f-2f0745b616e0"}, "merge_commit": {"hash": "63a65837af55", "type": "commit", "links": {"self": {"href": "data/repositories/ignitionrobotics/ign-transport/commit/63a65837af55.json"}, "html": {"href": "#!/ignitionrobotics/ign-transport/commits/63a65837af55"}}}, "closed_by": {"display_name": "Nate Koenig", "uuid": "{c862cdd9-fcc8-4419-9fc7-e20db14b8fcb}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D"}, "html": {"href": "https://bitbucket.org/%7Bc862cdd9-fcc8-4419-9fc7-e20db14b8fcb%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:095b1e12-74ed-4e20-b44f-2f0745b616e0/ca24bb11-4787-4b14-a20d-d91835e9bde2/128"}}, "nickname": "Nathan Koenig", "type": "user", "account_id": "557058:095b1e12-74ed-4e20-b44f-2f0745b616e0"}}